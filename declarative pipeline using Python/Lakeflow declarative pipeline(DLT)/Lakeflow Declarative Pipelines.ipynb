{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb669820-9a13-448d-aa16-e7f35ec4673b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### create external location & unity catalog \n",
    "### AUTO CDC API's replace the Apply changes API's\n",
    "### remove live its optional\n",
    "### Load the data to Landing zone and move it to Bronze layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a056ff04-0377-4ed8-a574-030abe580cb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Hive Metastore \n",
    "### create mount point\n",
    "### in case of Azure , create a adls account and container landing and directoreis orders & customers\n",
    "#### in databricks I have craeted a valume and upload data \n",
    "### A table can belongs to one pipeline only. otherwise it will create conflict in case of full refresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69b90db7-e10a-44a0-8869-ad4d5c4679ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "create streaming table if not exists orders_bronze_lakeflow_declaraive\n",
    "as\n",
    "select *,\n",
    "current_timestamp() as load_time,\n",
    "_metadata.file_name as file_name\n",
    "from stream(cloud_files('/Volumes/workspace/default/dlt_volume/input/orders', 'csv', map('cloud_files.inferColumnsTypes', 'true')));\n",
    "\n",
    "create streaming table if not exists customers_bronze_lakeflow_declaraive\n",
    "as\n",
    "select *, \n",
    "current_timestamp() as load_time,\n",
    "_metadata.file_name as file_name\n",
    "from stream(cloud_files('/Volumes/workspace/default/dlt_volume/input/customers', 'csv', map('cloud_files.inferColumnTypes', 'true')));\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cf21822-91b7-4dff-8e1c-7d4204f90190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2 : Take the data from Bronze and clean it and load it in the silver_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "515eac98-5f07-49fd-9fb4-5a120b3cd0f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "create streaming table if not exists orders_cleaned_silver_lakeflow_declaraive(\n",
    "constraint valid_orders expect(order_id is not null) on violation drop row,\n",
    "constraint valid_customers expect(customer_id is not null) on violation drop row\n",
    ")\n",
    "as\n",
    "select OrderID as order_id,\n",
    "OrderDate as order_date,\n",
    "CustomerID as customer_id,\n",
    "TotalAmount as total_amount,\n",
    "Status as order_status,\n",
    "file_name,\n",
    "load_time\n",
    "from \n",
    "stream(orders_bronze);\n",
    "\n",
    "create streaming table if not exists customers_cleaned_silver_lakeflow_declaraive(\n",
    "constraint valid_customers expect(customer_id is not null) on violation drop row\n",
    ")\n",
    "as\n",
    "select CustomerID as customer_id,\n",
    "CustomerName as customer_name,\n",
    "ContactNumber as contact_number,\n",
    "Email as email,\n",
    "Address as city,\n",
    "DateOfBirth as date_of_birth,\n",
    "RegistrationDate as resistration_date,\n",
    "file_name,\n",
    "load_time\n",
    "from \n",
    "stream(customers_bronze);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d13c624-9f5e-4dd7-9306-0717c0d0d2cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### take the data from cleaned silver and handle duplicates , scd 2 and load it to silver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "828f7053-43cc-42b1-b8c5-44f12adf7337",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "create streaming table orders_silver_lakeflow_declaraive;\n",
    "create flow orders_silver_flow as auto cdc\n",
    "into orders_silver_lakeflow_declaraive\n",
    "from stream(orders_cleaned_silver_lakeflow_declaraive)\n",
    "keys(order_id)\n",
    "sequence by load_time;\n",
    "\n",
    "create streaming table customer_silver_lakeflow_declaraive;\n",
    "create flow customers_silver_flow as auto cdc\n",
    "into customer_silver_lakeflow_declaraive\n",
    "from stream(customers_cleaned_silver_lakeflow_declaraive)\n",
    "keys(customer_id)\n",
    "sequence by load_time\n",
    "stored as scd type 2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68194cce-96e2-40e6-8746-161340e70284",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Gold Layer : Take the date from silver layer and perform aggregations and create Gold layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6b96fc6-95f1-4dfa-aa23-725a08084f4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "create materialized view if not exists city_wise_sales_gold_lakeflow_declaraive\n",
    "as\n",
    "select city , sum(total_amount) as total_sales\n",
    "from orders_silver_lakeflow_declaraive o\n",
    "join customer_silver_lakeflow_declaraive c\n",
    "on o.customer_id = c.customer_id\n",
    "group by city;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b36360b-ccd7-489a-aa0a-42d89e87a262",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### No upload new orders & customers files and you will see for customers(customer_id 80) it will perfomr scd type 2 and for orders (order_id 400) it will perform merge.\n",
    "### folders will be created for bronze , silver & Gold. \n",
    "### for Gold folder : Enzyme folder is also cretaed to prevent full load computation when incremental data arrives.\n",
    "## select * from orders_silver where order_id=400; > merge\n",
    "## select * from customer_silver where customer_id=80;  > scd type 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6ad4cf5-8704-4440-9440-3e2cf0bedf47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Lakeflow Declarative Pipelines",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
